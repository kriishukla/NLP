{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgmD5nPbQwKS",
        "outputId": "2708dd49-0ebd-4e56-8690-2c76a9343415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "\n",
        "# Download the movie reviews dataset from NLTK\n",
        "nltk.download('wordnet')\n",
        "nltk.download('movie_reviews') #(e.g., a collection of news articles) I am taking movie reviews for mine working\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9VDxrnGSxok",
        "outputId": "4f3eae59-92e3-410a-ec2d-32ac6331def0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Tokenization is the process of dividing a text into smaller units called tokens. These tokens can be words, sentences, or other meaningful units depending on the task or analysis being performed. Tokenization is a common preprocessing step in natural language processing (NLP) and text mining tasks, enabling the analysis and manipulation of text data at a more granular level."
      ],
      "metadata": {
        "id": "Um0Wo-Gkc41L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get user input\n",
        "user_input = input(\"Enter the text to tokenize: \")\n",
        "\n",
        "# Tokenize the user input\n",
        "tokens = nltk.word_tokenize(user_input)\n",
        "\n",
        "# Print the tokens\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql1K6KhtRHG1",
        "outputId": "1686d2b5-48e7-4864-ee63-c06f8574d648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text to tokenize: hello my name is krishna shukla whats yours\n",
            "Tokens: ['hello', 'my', 'name', 'is', 'krishna', 'shukla', 'whats', 'yours']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Stopwords are common words that are often considered insignificant or irrelevant in the analysis of text data. These words, such as \"the,\" \"and,\" \"a,\" and \"in,\" do not carry much semantic meaning and occur frequently across different documents or texts. Stopwords are often removed from the text during preprocessing to improve the efficiency and accuracy of natural language processing tasks, such as text classification, information retrieval, and sentiment analysis. Removing stopwords helps to reduce noise and focus on more meaningful words that carry the essence of the text."
      ],
      "metadata": {
        "id": "pb9aGelydLTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = input(\"Enter the text to tokenize and remove stop words: \")\n",
        "\n",
        "# Tokenize the user input\n",
        "tokens = nltk.word_tokenize(user_input)\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Print the filtered tokens\n",
        "print(\"Filtered Tokens:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zpmlC91ReW9",
        "outputId": "cffcc7c8-58f5-483d-b00a-b290777a09f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text to tokenize and remove stop words: and you are the one for whom i am living\n",
            "Filtered Tokens: ['one', 'living']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming: Stemming is the process of removing affixes from words to obtain their base form, known as the stem. This involves heuristics and rule-based approaches to remove common suffixes and prefixes. The resulting stems may not always be actual words, but they represent the core meaning of the word. Stemming helps to reduce word variations and consolidate related words.\n",
        "\n",
        "Lemmatization: Lemmatization, on the other hand, aims to determine the base form of a word, known as the lemma, using linguistic knowledge and morphological analysis. The lemma is a valid word that represents the canonical form of the word. Unlike stemming, lemmatization considers the context and part of speech (POS) of the word, ensuring that the resulting lemma is a valid word in the language. Lemmatization provides more accurate base forms, facilitating better understanding and analysis of text data."
      ],
      "metadata": {
        "id": "inHKo5lSdS0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get user input\n",
        "user_input = input(\"enter text to be stemmed or lemmatized\")\n",
        "\n",
        "# Tokenize the user input\n",
        "tokens = nltk.word_tokenize(user_input)\n",
        "\n",
        "# Initialize stemming and lemmatization tools\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform stemming\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token, wordnet.VERB) for token in tokens]\n",
        "\n",
        "# Print the stemmed tokens\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "\n",
        "# Print the lemmatized tokens\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmTtbQ5jRy5j",
        "outputId": "6e7584c6-b3b0-4304-b2b1-a2960e31498d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter text to be stemmed or lemmatizedThe quick brown fox jumps over the lazy dog\n",
            "Stemmed Tokens: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']\n",
            "Lemmatized Tokens: ['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counts the frequency of each word or phrase in the text.\n"
      ],
      "metadata": {
        "id": "troTx2nbda0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get user input\n",
        "user_input = input(\"Enter the text: \")\n",
        "\n",
        "# Tokenize the user input\n",
        "tokens = nltk.word_tokenize(user_input)\n",
        "\n",
        "# Create a frequency distribution of the tokens\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Print the frequency of each word or phrase\n",
        "print(\"Word/Phrase\\tFrequency\")\n",
        "for word, frequency in freq_dist.items():\n",
        "    print(f\"{word}\\t\\t{frequency}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dClJrXFSvWw",
        "outputId": "dd0834e6-a2b7-4a2c-9180-1efa26f1c197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text: hello hello hello my name is hello whats yours\n",
            "Word/Phrase\tFrequency\n",
            "hello\t\t4\n",
            "my\t\t1\n",
            "name\t\t1\n",
            "is\t\t1\n",
            "whats\t\t1\n",
            "yours\t\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most Frequent word\n"
      ],
      "metadata": {
        "id": "UxcwHvDMdnoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Get user input\n",
        "user_input = input(\"Enter the text: \")\n",
        "\n",
        "# Tokenize the user input\n",
        "tokens = nltk.word_tokenize(user_input)\n",
        "\n",
        "# Create a frequency distribution of the tokens\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Get the most common word and its frequency\n",
        "most_common_word, frequency = freq_dist.most_common(1)[0]\n",
        "\n",
        "# Print the most frequent word and its frequency\n",
        "print(\"Most Frequent Word:\", most_common_word)\n",
        "print(\"Frequency:\", frequency)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poLssFTUSwa8",
        "outputId": "29b51b87-305d-47d6-d737-fef081985937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text: hello hello my name is krishna shukla\n",
            "Most Frequent Word: hello\n",
            "Frequency: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XwNIN0FBcz0m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}